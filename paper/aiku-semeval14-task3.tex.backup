\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{multirow}
\usepackage{latexsym}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{graphicx}





%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\title{AI-KU: Paper Title}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

In this paper, we describe our two unsupervised systems that based on co-occurrence statistics of words. There is only one difference between the systems is that the input they use. 

\end{abstract}

\section{Introduction}
\label{intro}

Semantic similarity is a measure that specifies the similarity of the meaning of one text to another. Semantic similarity plays an important role in many Natural Language Processing (NLP) tasks ranging from textual entailment to summarization, question answering to document categorization. 

There are three main approaches to compute text similarity. \emph{Refer to VSM paper?}

In this paper, we describe our two unsupervised systems that based on co-occurrence statistics of words. There is only one difference between the systems is that the input they use. First system uses the words directly (after lemmatization, stopword removing and excluding the non-alphanumeric characters) in text while the second system utilizes the most likely substitutes consulted by a 4-gram language model for each observed word position (i.e., context).

The remainder of the paper proceeds as follows. Section \ref{algorithm} presents the preprocessing part, the difference between the systems, co-occurrence modeling, and how we calculate the similarity between two texts after co-occurrence modeling has been done. Section \ref{evaluation} discusses the results of our systems and makes a comparison with other participants. Section \ref{conclusion} concludes the findings and ends with a short future work.

% Semantic measures play an important role to compare such elements according to semantic proxies: texts and knowledge representations, which support their meaning or describe their nature.

% Semantic similarity is the practical, widely used approach to address the natural language understanding issue in many core NLP tasks such as paraphrase identification, Question Answering, Natural Language Generation, and Intelligent Tutoring Systems. %In the semantic similarity approach, the meaning of a target text is inferred by assessing how similar it is to another text, called the benchmark text, whose meaning is known. If the two texts are similar enough, according to some measure of semantic similarity, the meaning of the target text is deemed similar to the meaning of the benchmark text.

% Measuring semantic text similarity has been a re- % search subject in natural language processing, infor- % mation retrieval and artificial intelligence for many % years. Previous efforts have focused on compar- % ing two long texts (e.g., for document classification) % or a short text with a long text (e.g., Web search), % but there are a growing number of tasks requiring % computing the semantic similarity between two sen- % tences or other short text sequences. They include % paraphrase recognition (Dolan et al., 2004), Twitter % tweets search (Sriram et al., 2010), image retrieval % by captions (Coelho et al., 2004), query reformula- % tion (Metzler et al., 2007), automatic machine trans- % lation evaluation (Kauchak and Barzilay, 2006) and % schema matching (Han et al., 2012). % There are three predominant approaches to com- % puting short text similarity. The first uses informa- % tion retrieval’s vector space model (Meadow, 1992) % in which each text is modeled as a “bag of words”

% Comparing textual data to establish the degree of se- mantic similarity is of key importance in many Nat- ural Language Processing (NLP) tasks ranging from document categorization to textual entailment and summarization. The key aspect of having an accu- rate STS framework is the design of features that can adequately represent various aspects of the similar- ity between texts, e.g. using lexical, syntactic and semantic similarity metrics. The majority of approaches to semantic textual similarity treat the input text pairs as feature vec- tors where each feature is a score corresponding to a certain type of similarity.


%To this end

% Unlike prior SemEval tasks on textual similarity that have focused on comparing similar-sized texts, this task evaluates the case where larger text must be compared to smaller text.
%Task participants will be provided with pairs of each comparison type and asked to rate the pair according to the semantic similarity of the smaller item to the larger item

%Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing: 

\begin{table}
\begin{center}
\begin{tabular}{|r|c|}
\hline \bf ID & \bf Token \\ \hline
R2 & how \\
R2 & blog \\
R2 & popular \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:system1_input} Input example for AI-KU$_1$.}
\end{table}


\begin{table*}
  \resizebox{\textwidth}{!}{
  \centering
  \begin{tabular}{r||c||c}
    \toprule
    % \cmidrule{2-3} \cmidrule{3-3}
               Word & Context & Substitutes \\
                \hline
the & $<$s$>$ \_\_\_ dog & $w_1$ (0.01), $w_2$ (0.53),  $w_3$ (0.13), ..., $w_n$ (0.02) \\ 
dog & the \_\_\_ bites & $w_1$ (0.01), $w_2$ (0.1),  $w_3$ (0.05), ..., $w_n$ (0.01) \\ 
bites & dog \_\_\_ $<$/s$>$ & $w_1$ (0.03), $w_2$ (0.23),  $w_3$ (0.04), ..., $w_n$ (0.01) \\
\bottomrule
  \end{tabular}}
\caption{Contexts when using a bigram language model}
\label{tab:subs_exp}
\end{table*}

\section{Algorithm}
\label{algorithm}
In this section, we will explain preprocessing steps of the data and the details of our two participated systems\footnote{The code to replicate our work can be found \emph{input github repo link here}.} for this task. Both systems rely on the co-occurrence statistics. The slight difference between the first and the second system is that the first one uses the words occur in the text (e.g., paragraph, sentence etc.), whereas the second system employs co-occurrence statistics on 100 most likely substitutes of each occurred word in text. 

\subsection{Data Preprocessing}

\begin{table}[t]
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline & \bf System & \bf Pearson & \bf Spearman  \\ 
% \hline
% \multicolumn{5}{|c|}{Test Set} \\
\hline
\multirow{9}{*}{\rotatebox{90}{\small{Paragraph-2-Sentence}}}
& Best & 0.837 & 0.821 \\
& $2^{nd}$ Best & 0.834 & 0.820 \\
& $3^{rd}$ Best & 0.826 & 0.817 \\
& AI-KU$_1$ & 0.732 & 0.727 \\
& AI-KU$_2$ & 0.698 & 0.700 \\
& LCS & 0.527 & 0.613 \\ 
& lch & 0.125 & 0.114 \\
& jcn & 0.107 & 0.106 \\
& JI & 0.640  & 0.687 \\ \hline
\end{tabular}}
\end{center}
\caption{\label{table:results-p2s} Paragraph-2-Sentence subtask scores for the test set. \emph{Best} indicates the best score for the subtasks. LCS stands for Normalized Longest Common Substring. Subscripts in AI-KU systems indicate the run number.}
\end{table}

\begin{table}[t]
\begin{center}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline & \bf System & \bf Pearson & \bf Spearman  \\ 
% \hline
% \multicolumn{5}{|c|}{Test Set} \\
\hline
\multirow{9}{*}{\rotatebox{90}{\small{Sentence-2-Phrase}}}
& Best & 0.777 & 0.642 \\
& $2^{nd}$ Best & 0.771 & 0.760 \\
& $3^{rd}$ Best & 0.760 & 0.757 \\
& AI-KU$_1$ & 0.680 & 0.646 \\
& AI-KU$_2$ & 0.617 & 0.612 \\ 
& LCS & 0.562 & 0.626 \\ 
& lch & 0.420 & 0.431 \\
& jcn & 0.469 & 0.535 \\
& JI & 0.540 & 0.555 \\ \hline
\end{tabular}}
\end{center}
\caption{\label{table:results-s2p} Sentence2phrase subtask scores for the test set. \emph{Best} indicates the best score for the subtasks. LCS stands for Normalized Longest Common Substring. Subscripts in AI-KU systems indicate the run number.}
\end{table}

Two AI-KU systems can be distinguished between each other on input of co-occurrence modeling part. 

\paragraph{AI-KU$_1$:} This system uses the words that observed in the text. All words are translated into lower-case equivalents. Lemmatization\footnote{Lemmatization is carried out with Stanford CoreNLP and transforms a word into its canonical or base form.}, stop-word removing has been done and non-alphanumeric characters are excluded. Table \ref{tab:system1_input} shows the input of the AI-KU$_1$ for the following sentence which is an instance from paragraph2sentence test set:

\begin{quote}
How can my blog be more popular?
\end{quote}



\paragraph{AI-KU$_2$:} Unlike the first system, this system represents each context of a word by finding the most likely substitutes suggested by the 4-gram language model we built from ukWaC\footnote{Available here: http://wacky.sslmit.unibo.it} \cite{ukWaC}, a 2-billion word web-gathered corpus. High probability substitutes worked successfully on Word Sense Induction (WSI) \cite{baskaya13ai} and Part-of-Speech Induction \cite{yatbaz2012learning} problems. We used the 100 most likely substitute for each context. No lemmatization, stop word removing and lower-case transformation has been done. Table~\ref{tab:subs_exp} illustrates the context and substitutes of each context using a bigram language model.



\subsection{Co-Occurrence Modeling}

This subsection will explain the unsupervised methods we employed to model co-occurrence statistics: the Co-occurrence data Embedding (CODE) method \cite{globerson-CODE} and its spherical extension (S-CODE) proposed by Maron et al. \shortcite{Maron2010}. Unlike our previous WSI work \cite{baskaya13ai} where we ended up with an embedding for each
word in the co-occurrence modeling step, in this task however, we try to model each text unit such as a paragraph, a sentence or a phrase, thus, after this step, we obtain embeddings for each paragraph, sentence and phrase instance. 

Input data for S-CODE algorithm were instance-id of the text unit and the each word in the text unit for the first system (see Table \ref{tab:system1_input}), the 100 most likely substitutes of each word in text for the second system. In the initial step, S-CODE puts all instance-ids and words/substitutes randomly on n-dimensional sphere. If two different instances have the same word/substitute, then these two instances attract each other, otherwise they will repel. When S-CODE converges, instances have similar words/substitutes will be closely located, instances have no similar words/substitutes will be far from each other. 

\paragraph{AI-KU$_1$:} According to the training set performances for various $n$ (i.e., number of dimensions for S-CODE algorithm), we picked 100 for both tasks.

\paragraph{AI-KU$_2$:} Prior performance tests on training set, we picked $n$ equals to 200 and 100 for paragraph2sentence and sentence2phrase subtasks, respectively.

\subsection{Similarity Calculation}

When the S-CODE converges, we have an n-dimensional embedding for each textual level (e.g., paragraph, sentence, phrase) instance. We can use a similarity metric to calculate 
the similarity between embeddings. For this task, systems should report only the similarity between two specific cross level instances. Note that we used cosine similarity in order to calculate similarity between two textual unit. \emph{How about the other metrics? Make a test and input the reason why we use cosine similarity}. This similarity is the eventual similarity for two instances; no further processing (e.g., scaling) has been done.

% Bag of words
% Dimensionlar farkliydi iki subtask icin.
% Unsupervised oldugumuzu dile getirelim. Baslikta filan da dile getirilebilir.
% 2 task'e katildik, bunlar p2s s2p. Context enrichment yapildi ukwac ile fakat training'te daha kotu sonuclar elde edildi. 
% Stanford CoreNLP kullanildi lemmatize edilirken.
% Iki deney yapildi, biri sentence icindeki kelimeleri vererek yakinlik hesaplamak, digeri once substitute'larini onlari kullanarak iki text'in yakinliginin hesaplanmasi, LM olarak ukwac kullanildi (detaylari gir), 100 subs alindi, alphanumeric olmayan karakterlerden temizlendi (makefile'a bak gen-p2s-input targeti. Sonradan edit galiba bu alakasiz ir sey) lowercase'e gecildi
% Jaccard index baseline'indan ve diger baseline'lardan bahsedilebilir.
% Training setteki sonuclar ve test setteki sonuclara ait tablo,
% Ayri ayri genre scorelari uzerinden tablo. David burada bi genre'de cok iyi yaptigimizi soylemisti.
% Enrichment yapilmadi, ama test datasinda sonuclar alinirken training datasi da kullanildi.
% Yer kalmazsa training data'daki skorlari supplementary material kisvesi altinda ver.
% w1 w2 w3 context tablosunda. Onlari kelimelerle degistirin.
% text fragments kullanilabilir. Estimating the semantic relatedness of two text fragments – such as words, sentences, or entire documents – is important for many natural language processing or information retrieval applications.


\section{Evaluation Results}
\label{evaluation}

In this task, systems were evaluated with two correlation metrics; Pearson correlation and Spearman's rank correlation.  Pearson correlation tests the degree of similarity between the system's similarity ratings and the gold standard ratings.  Spearman's rho tests the degree of similarity between the rankings of the items according to similarity.

Table \ref{table:results-p2s} and \ref{table:results-s2p} show the scores for Paragraph-2-Sentence and Sentence-2-Phrase subtasks, respectively. These tables contain the best individual scores for the performance metrics, Normalized Longest Common Substring (LCS) baseline which is given by task organizers and three additional baselines: lch \cite{leacock1998combining}, jcn \cite{jiang1997semantic}, and Jaccard Index (JI) baseline. jcn uses information content \cite{resnik1995inforcontent} of the least common subsumer  of concepts A and B. Information content indicates the specificity of a concept and least common subsumer of a concept A and B is the most specific concept from which A and B inherited. \emph{Paraphrase: jcn measure augments the information content of the least common subsumer with the sum of the information content of concepts A and B themselves and takes the difference of this sum and the information content of the least common subsumer.} On the other hand, \emph{Paraphrase: lch is based on 
path lengths between a pair of concepts. It finds the shortest path between two concepts, and scales that value by the maximum path length found in the is–a hierarchy in which they occur} (please see Pedersen et al. \shortcite{pedersen2004wordnet} for further details of these measures). These two baselines were calculated as follows. First, using the Stanford Part-of-Speech Tagger \cite{toutanova2000enriching} we tagged words in all textual levels. After tagging, we found the synsets of each word matched with its part-of-speech using WordNet 3.0 \cite{fellbaum98electronic}. For each synset of a word in the shorter textual unit (e.g., sentence is shorter than paragraph), we calculated the jcn/lch measure of each synset of all words in the longer textual unit and picked the highest score. When we found the scores for all words, we sum them up and normalized it with the length of the shorter textual unit. This gave us the similarity between one pair in the test set. Finally, Jaccard Index baseline simply 
calculates the number of words in common (intersection) with two cross textual levels, normalized by the total number of words (union). 

\paragraph{Paragraph2Sentence:} Both systems outperformed all the baselines for both metrics. The best score for this subtask was .837 and our systems achieved .732 and .698 on Pearson and did similar on Spearman metric. These scores are promising since our current unsupervised systems are based on bag-of-word approach; they do not utilize any syntactic information (e.g., \emph{input here with some examples}). Jaccard Index baseline score was .640. \emph{Investigate why wordnet baselines are very low. Input the reason}.

\paragraph{Sentence2Phrase:} In this subtask, AI-KU systems outperformed all baselines except that AI-KU$_2$ system did slightly worse than LCS on Spearman metric. Performances of systems and baselines were lower than Paragraph2Sentence subtask, since smaller textual unit (i.e., phrase) make the problem harder.


% genre information'inina gore karsilastirma yapilabilir?


\section{Conclusion}
\label{conclusion}

In this work, we presented two unsupervised systems that utilize co-occurrence statistics and represent textual units as a dense, low dimensional embeddings. 
Although current systems are based on bag-of-word approach and discard the syntactic information, they achieved promising results both paragraph2sentence and sentence2phrase subtasks. \emph{Add some sentences to be an explanation for the prior work that didn't perform well.} As a future work, we will extend our systems by adding syntactic information \emph{dependency parsing information? But, then we'll no longer be unsupervised?} into S-CODE.  

% syntactic information'a bakilmadan iyi sonuclar elde edildi, future work olarak SCODE'a syntax'in da handle edilebilecegi feature'lar eklenecek.

% \ref, \noindent, \shortcite, \cite, \url

\section*{Acknowledgements}

\emph{You may want to thank Tubitak.}

\newpage
% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{references}



\end{document}
